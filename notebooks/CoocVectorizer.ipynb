{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CoocVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math, joblib\n",
    "from deepART import base, nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without Filter\n",
    "\n",
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building Vocab-Index Dictionary...\n",
      "Number of Distinct Words:  20 \n",
      "\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Converting All Matrix Entries to Mutual Information Estimates...\n",
      "Done...\n",
      "\n",
      "Normalizing Matrix...\n",
      "Done...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/deepART/nlp/cooc_vec/cooccurrence.py:224: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  computed = P_X_GIVEN_Y / P_X\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac143d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I love you I hate you'.split(), 'You are a piece of shit and a god'.split(), 'Facebook is life I am god'.split(),\n",
    "               'I love you so much I gonna die'.split(), 'I love god and hate Facebook'.split()]\n",
    "trial_vectorizer = nlp.cooc_vec.CoocVectorizer()\n",
    "trial_vectorizer.fit(trial_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 4., 0., 2., 1., 0., 0.,\n",
       "        0., 2., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 2., 0., 1., 0., 0., 1.,\n",
       "        0., 1., 2., 0.],\n",
       "       [1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
       "        1., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0.],\n",
       "       [1., 0., 4., 2., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
       "        1., 4., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "        1., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0.],\n",
       "       [0., 0., 2., 1., 0., 0., 0., 0., 0., 0., 4., 0., 0., 1., 0., 0.,\n",
       "        0., 0., 1., 1.],\n",
       "       [0., 1., 0., 2., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer.counts_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.60382138,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.32296527, 0.7287614 , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.45694698, 0.54859154, 0.54859154,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.43507943, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.42798595,\n",
       "        0.4647404 , 0.        , 0.35648907, 0.        , 0.        ,\n",
       "        0.22891637, 0.        , 0.37618346, 0.42798595, 0.        ,\n",
       "        0.        , 0.        , 0.31092004, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.41194909, 0.        , 0.        ,\n",
       "        0.26452954, 0.        , 0.43470739, 0.        , 0.        ,\n",
       "        0.53704139, 0.        , 0.35929076, 0.39223495, 0.        ],\n",
       "       [0.63580361, 0.        , 0.39349706, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.31317667, 0.        , 0.        , 0.58552046, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.45881826, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.56866694, 0.68271788, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.28018028, 0.        , 0.        , 0.        , 0.52382983,\n",
       "        0.56881514, 0.56881514, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.4120868 , 0.27694155, 0.32681964, 0.        ,\n",
       "        0.44747586, 0.        , 0.        , 0.4120868 , 0.4120868 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.32681964, 0.        ],\n",
       "       [0.        , 0.50813631, 0.        , 0.        , 0.        ,\n",
       "        0.5517739 , 0.        , 0.42324997, 0.        , 0.50813631,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.54859154, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.45694698, 0.54859154, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.43507943, 0.        ],\n",
       "       [0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "        0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "        0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648],\n",
       "       [0.89707803, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.44187216, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.38828485, 0.45821624, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.30902838, 0.        , 0.        , 0.        , 0.57776474,\n",
       "        0.        , 0.        , 0.        , 0.45821624, 0.        ],\n",
       "       [0.        , 0.        , 0.44649642, 0.        , 0.66438308,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.35535784, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.48265607, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.52846565, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.46450132, 0.        , 0.        ,\n",
       "        0.        , 0.57384907, 0.        , 0.4191179 , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.573096  , 0.        ,\n",
       "        0.        , 0.7226166 , 0.        , 0.        , 0.        ,\n",
       "        0.38650514, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.66138424, 0.        , 0.        , 0.        ,\n",
       "        0.35375385, 0.        , 0.        , 0.        , 0.66138424,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.31849685, 0.37585918, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.25348546, 0.        , 0.        , 0.47392074, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.37585918, 0.57198231],\n",
       "       [0.        , 0.42170756, 0.        , 0.33444972, 0.        ,\n",
       "        0.        , 0.        , 0.35125951, 0.        , 0.42170756,\n",
       "        0.        , 0.        , 0.37066499, 0.        , 0.42170756,\n",
       "        0.        , 0.        , 0.30635896, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.59289246, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.80528165, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gonna': 0,\n",
       " 'shit': 1,\n",
       " 'you': 2,\n",
       " 'god': 3,\n",
       " 'much': 4,\n",
       " 'are': 5,\n",
       " 'life': 6,\n",
       " 'a': 7,\n",
       " 'piece': 8,\n",
       " 'of': 9,\n",
       " 'i': 10,\n",
       " 'die': 11,\n",
       " 'hate': 12,\n",
       " 'so': 13,\n",
       " 'facebook': 14,\n",
       " 'am': 15,\n",
       " 'is': 16,\n",
       " 'love': 17,\n",
       " 'and': 18,\n",
       " 'hello': 19}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab-Index Dictionary...\n",
      "Number of Distinct Words:  4 \n",
      "\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Extracting Cooccurrence Vectors...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "         0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "         0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648]),\n",
       "  array([0.        , 0.        , 0.31849685, 0.37585918, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.25348546, 0.        , 0.        , 0.47392074, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.37585918, 0.57198231]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.42798595,\n",
       "         0.4647404 , 0.        , 0.35648907, 0.        , 0.        ,\n",
       "         0.22891637, 0.        , 0.37618346, 0.42798595, 0.        ,\n",
       "         0.        , 0.        , 0.31092004, 0.        , 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.41194909, 0.        , 0.        ,\n",
       "         0.26452954, 0.        , 0.43470739, 0.        , 0.        ,\n",
       "         0.53704139, 0.        , 0.35929076, 0.39223495, 0.        ])]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer.transform(['I love you god'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: [0.31568816 0.         0.19537851 0.23056683 0.29072165 0.\n",
      " 0.29072165 0.         0.         0.         0.         0.35087648\n",
      " 0.25553334 0.29072165 0.         0.31568816 0.31568816 0.2112013\n",
      " 0.         0.35087648]\n",
      "\n",
      "love: [0.         0.         0.31849685 0.37585918 0.         0.\n",
      " 0.         0.         0.         0.         0.25348546 0.\n",
      " 0.         0.47392074 0.         0.         0.         0.\n",
      " 0.37585918 0.57198231]\n",
      "\n",
      "you: [0.         0.         0.         0.         0.42798595 0.4647404\n",
      " 0.         0.35648907 0.         0.         0.22891637 0.\n",
      " 0.37618346 0.42798595 0.         0.         0.         0.31092004\n",
      " 0.         0.        ]\n",
      "\n",
      "god: [0.         0.         0.         0.         0.         0.\n",
      " 0.         0.41194909 0.         0.         0.26452954 0.\n",
      " 0.43470739 0.         0.         0.53704139 0.         0.35929076\n",
      " 0.39223495 0.        ]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('i: {}\\n'.format(trial_vectorizer.matrix[trial_vectorizer.vocabulary['i'], :]))\n",
    "print('love: {}\\n'.format(trial_vectorizer.matrix[trial_vectorizer.vocabulary['love'], :]))\n",
    "print('you: {}\\n'.format(trial_vectorizer.matrix[trial_vectorizer.vocabulary['you'], :]))\n",
    "print('god: {}\\n'.format(trial_vectorizer.matrix[trial_vectorizer.vocabulary['god'], :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab-Index Dictionary...\n",
      "Number of Distinct Words:  20 \n",
      "\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Converting All Matrix Entries to Mutual Information Estimates...\n",
      "Done...\n",
      "\n",
      "Normalizing Matrix...\n",
      "Done...\n",
      "\n",
      "Extracting Cooccurrence Vectors...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.59289246, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.80528165, 0.        , 0.        ]),\n",
       "  array([0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "         0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "         0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648]),\n",
       "  array([0.        , 0.        , 0.31849685, 0.37585918, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.25348546, 0.        , 0.        , 0.47392074, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.37585918, 0.57198231]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.42798595,\n",
       "         0.4647404 , 0.        , 0.35648907, 0.        , 0.        ,\n",
       "         0.22891637, 0.        , 0.37618346, 0.42798595, 0.        ,\n",
       "         0.        , 0.        , 0.31092004, 0.        , 0.        ]),\n",
       "  array([0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "         0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "         0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648]),\n",
       "  array([0.        , 0.        , 0.38828485, 0.45821624, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.30902838, 0.        , 0.        , 0.        , 0.57776474,\n",
       "         0.        , 0.        , 0.        , 0.45821624, 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.42798595,\n",
       "         0.4647404 , 0.        , 0.35648907, 0.        , 0.        ,\n",
       "         0.22891637, 0.        , 0.37618346, 0.42798595, 0.        ,\n",
       "         0.        , 0.        , 0.31092004, 0.        , 0.        ])],\n",
       " [array([0.        , 0.        , 0.        , 0.        , 0.42798595,\n",
       "         0.4647404 , 0.        , 0.35648907, 0.        , 0.        ,\n",
       "         0.22891637, 0.        , 0.37618346, 0.42798595, 0.        ,\n",
       "         0.        , 0.        , 0.31092004, 0.        , 0.        ]),\n",
       "  array([0.        , 0.        , 0.45881826, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.56866694, 0.68271788, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.        , 0.4120868 , 0.27694155, 0.32681964, 0.        ,\n",
       "         0.44747586, 0.        , 0.        , 0.4120868 , 0.4120868 ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.32681964, 0.        ]),\n",
       "  array([0.        , 0.50813631, 0.        , 0.        , 0.        ,\n",
       "         0.5517739 , 0.        , 0.42324997, 0.        , 0.50813631,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.        , 0.54859154, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.45694698, 0.54859154, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.43507943, 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.45694698, 0.54859154, 0.54859154,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.43507943, 0.        ]),\n",
       "  array([0.        , 0.42170756, 0.        , 0.33444972, 0.        ,\n",
       "         0.        , 0.        , 0.35125951, 0.        , 0.42170756,\n",
       "         0.        , 0.        , 0.37066499, 0.        , 0.42170756,\n",
       "         0.        , 0.        , 0.30635896, 0.        , 0.        ]),\n",
       "  array([0.        , 0.4120868 , 0.27694155, 0.32681964, 0.        ,\n",
       "         0.44747586, 0.        , 0.        , 0.4120868 , 0.4120868 ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.32681964, 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.41194909, 0.        , 0.        ,\n",
       "         0.26452954, 0.        , 0.43470739, 0.        , 0.        ,\n",
       "         0.53704139, 0.        , 0.35929076, 0.39223495, 0.        ])],\n",
       " [array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52846565, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.46450132, 0.        , 0.        ,\n",
       "         0.        , 0.57384907, 0.        , 0.4191179 , 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.66138424, 0.        , 0.        , 0.        ,\n",
       "         0.35375385, 0.        , 0.        , 0.        , 0.66138424,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.28018028, 0.        , 0.        , 0.        , 0.52382983,\n",
       "         0.56881514, 0.56881514, 0.        , 0.        , 0.        ]),\n",
       "  array([0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "         0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "         0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648]),\n",
       "  array([0.        , 0.        , 0.        , 0.573096  , 0.        ,\n",
       "         0.        , 0.7226166 , 0.        , 0.        , 0.        ,\n",
       "         0.38650514, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.41194909, 0.        , 0.        ,\n",
       "         0.26452954, 0.        , 0.43470739, 0.        , 0.        ,\n",
       "         0.53704139, 0.        , 0.35929076, 0.39223495, 0.        ])],\n",
       " [array([0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "         0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "         0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648]),\n",
       "  array([0.        , 0.        , 0.31849685, 0.37585918, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.25348546, 0.        , 0.        , 0.47392074, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.37585918, 0.57198231]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.42798595,\n",
       "         0.4647404 , 0.        , 0.35648907, 0.        , 0.        ,\n",
       "         0.22891637, 0.        , 0.37618346, 0.42798595, 0.        ,\n",
       "         0.        , 0.        , 0.31092004, 0.        , 0.        ]),\n",
       "  array([0.        , 0.        , 0.44649642, 0.        , 0.66438308,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.35535784, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.48265607, 0.        , 0.        ]),\n",
       "  array([0.63580361, 0.        , 0.39349706, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.31317667, 0.        , 0.        , 0.58552046, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "         0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "         0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.60382138,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.32296527, 0.7287614 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]),\n",
       "  array([0.89707803, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.44187216, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ])],\n",
       " [array([0.31568816, 0.        , 0.19537851, 0.23056683, 0.29072165,\n",
       "         0.        , 0.29072165, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.35087648, 0.25553334, 0.29072165, 0.        ,\n",
       "         0.31568816, 0.31568816, 0.2112013 , 0.        , 0.35087648]),\n",
       "  array([0.        , 0.        , 0.31849685, 0.37585918, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.25348546, 0.        , 0.        , 0.47392074, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.37585918, 0.57198231]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.41194909, 0.        , 0.        ,\n",
       "         0.26452954, 0.        , 0.43470739, 0.        , 0.        ,\n",
       "         0.53704139, 0.        , 0.35929076, 0.39223495, 0.        ]),\n",
       "  array([0.        , 0.42170756, 0.        , 0.33444972, 0.        ,\n",
       "         0.        , 0.        , 0.35125951, 0.        , 0.42170756,\n",
       "         0.        , 0.        , 0.37066499, 0.        , 0.42170756,\n",
       "         0.        , 0.        , 0.30635896, 0.        , 0.        ]),\n",
       "  array([0.        , 0.        , 0.38828485, 0.45821624, 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.30902838, 0.        , 0.        , 0.        , 0.57776474,\n",
       "         0.        , 0.        , 0.        , 0.45821624, 0.        ]),\n",
       "  array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.52846565, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.46450132, 0.        , 0.        ,\n",
       "         0.        , 0.57384907, 0.        , 0.4191179 , 0.        ])]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I love you I hate you'.split(), 'You are a piece of shit and a god'.split(), 'Facebook is life I am god'.split(),\n",
    "               'I love you so much I gonna die'.split(), 'I love god and hate Facebook'.split()]\n",
    "trial_vectorizer_1 = nlp.cooc_vec.CoocVectorizer()\n",
    "trial_vectorizer_1.fit_transform(trial_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming, incremental fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building Vocab-Index Dictionary...\n",
      "Number of Distinct Words:  20 \n",
      "\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Converting All Matrix Entries to Mutual Information Estimates...\n",
      "Done...\n",
      "\n",
      "Normalizing Matrix...\n",
      "Done...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac49fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I love you I hate you'.split(), 'You are a piece of shit and a god'.split(), 'Facebook is life I am god'.split(),\n",
    "               'I love you so much I gonna die'.split(), 'I love god and hate Facebook'.split()]\n",
    "trial_vectorizer_2 = nlp.cooc_vec.CoocVectorizer()\n",
    "trial_vectorizer_2.fit(trial_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gonna': 0,\n",
       " 'shit': 1,\n",
       " 'you': 2,\n",
       " 'god': 3,\n",
       " 'much': 4,\n",
       " 'are': 5,\n",
       " 'life': 6,\n",
       " 'a': 7,\n",
       " 'piece': 8,\n",
       " 'of': 9,\n",
       " 'i': 10,\n",
       " 'die': 11,\n",
       " 'hate': 12,\n",
       " 'so': 13,\n",
       " 'facebook': 14,\n",
       " 'am': 15,\n",
       " 'is': 16,\n",
       " 'love': 17,\n",
       " 'and': 18,\n",
       " 'hello': 19}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_2.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20) (20, 20)\n"
     ]
    }
   ],
   "source": [
    "print(trial_vectorizer_2.matrix.shape, trial_vectorizer_2.counts_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Updating Vocabulary With New Words...\n",
      "Done...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Updating Cooccurrence Matrix...\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n",
      "Converting All Matrix Entries to Mutual Information Estimates...\n",
      "Done...\n",
      "\n",
      "Normalizing Matrix...\n",
      "Done...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac49fd0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_2.fit(['She has curves'.split()]) # 3 new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gonna': 0,\n",
       " 'shit': 1,\n",
       " 'you': 2,\n",
       " 'god': 3,\n",
       " 'much': 4,\n",
       " 'are': 5,\n",
       " 'life': 6,\n",
       " 'a': 7,\n",
       " 'piece': 8,\n",
       " 'of': 9,\n",
       " 'i': 10,\n",
       " 'die': 11,\n",
       " 'hate': 12,\n",
       " 'so': 13,\n",
       " 'facebook': 14,\n",
       " 'am': 15,\n",
       " 'is': 16,\n",
       " 'love': 17,\n",
       " 'and': 18,\n",
       " 'hello': 19,\n",
       " 'she': 20,\n",
       " 'curves': 21,\n",
       " 'has': 22}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_2.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23, 23) (23, 23)\n"
     ]
    }
   ],
   "source": [
    "print(trial_vectorizer_2.matrix.shape, trial_vectorizer_2.counts_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With Filter\n",
    "\n",
    "### fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac42f90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I I love love you you I I hate hate you you'.split(), 'You You are are a a piece piece of of shit shit and and a a god god'.split(), 'Facebook Facebook is is life life I I am am god god'.split(), 'I I love love you you so so much much I I gonna gonna die die'.split()]\n",
    "trial_vectorizer_3 = nlp.cooc_vec.CoocVectorizer(min_df=2, max_df=4, cooc_likelihood=False, normalize=None) #counts only\n",
    "trial_vectorizer_3.fit(trial_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 0,\n",
       " 'hate': 1,\n",
       " 'are': 2,\n",
       " 'a': 3,\n",
       " 'piece': 4,\n",
       " 'of': 5,\n",
       " 'shit': 6,\n",
       " 'and': 7,\n",
       " 'god': 8,\n",
       " 'facebook': 9,\n",
       " 'is': 10,\n",
       " 'life': 11,\n",
       " 'am': 12,\n",
       " 'so': 13,\n",
       " 'much': 14,\n",
       " 'gonna': 15,\n",
       " 'die': 16}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_3.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1,\n",
       " 'i': 10,\n",
       " 'love': 4,\n",
       " 'you': 8,\n",
       " 'hate': 2,\n",
       " 'are': 2,\n",
       " 'a': 4,\n",
       " 'piece': 2,\n",
       " 'of': 2,\n",
       " 'shit': 2,\n",
       " 'and': 2,\n",
       " 'god': 4,\n",
       " 'facebook': 2,\n",
       " 'is': 2,\n",
       " 'life': 2,\n",
       " 'am': 2,\n",
       " 'so': 2,\n",
       " 'much': 2,\n",
       " 'gonna': 2,\n",
       " 'die': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_3.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rejected Words Are dict_keys(['hello', 'i', 'you'])\n"
     ]
    }
   ],
   "source": [
    "print('Rejected Words Are {}'.format(trial_vectorizer_3.rejected_words.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 17) (17, 17)\n"
     ]
    }
   ],
   "source": [
    "print(trial_vectorizer_3.counts_matrix.shape, trial_vectorizer_3.matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Vocab-Index Dictionary...\n",
      "Number of Distinct Words:  3 \n",
      "\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Extracting Cooccurrence Vectors...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0.])]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_3.transform(['Facebook is life'.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "is: [0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 3. 0. 0. 0. 0. 0.]\n",
      "\n",
      "life: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('facebook: {}\\n'.format(trial_vectorizer_3.matrix[trial_vectorizer_3.vocabulary['facebook'], :]))\n",
    "print('is: {}\\n'.format(trial_vectorizer_3.matrix[trial_vectorizer_3.vocabulary['is'], :]))\n",
    "print('life: {}\\n'.format(trial_vectorizer_3.matrix[trial_vectorizer_3.vocabulary['life'], :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Extracting Cooccurrence Vectors...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 3., 0., 3., 0., 0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 3., 0., 3., 0., 0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 3., 0., 3., 0., 0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 3., 0., 3., 0., 0., 3., 3., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0.])],\n",
       " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 3., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 3., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0., 0.])],\n",
       " [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0.]),\n",
       "  array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0.])]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I I love love you you I I hate hate you you'.split(), 'You You are are a a piece piece of of shit shit and and a a god god'.split(), 'Facebook Facebook is is life life I I am am god god'.split(), 'I I love love you you so so much much I I gonna gonna die die'.split()]\n",
    "trial_vectorizer_3 = nlp.cooc_vec.CoocVectorizer(min_df=2, max_df=4, cooc_likelihood=False, normalize=None) #counts only\n",
    "trial_vectorizer_3.fit_transform(trial_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 0,\n",
       " 'hate': 1,\n",
       " 'are': 2,\n",
       " 'a': 3,\n",
       " 'piece': 4,\n",
       " 'of': 5,\n",
       " 'shit': 6,\n",
       " 'and': 7,\n",
       " 'god': 8,\n",
       " 'facebook': 9,\n",
       " 'is': 10,\n",
       " 'life': 11,\n",
       " 'am': 12,\n",
       " 'so': 13,\n",
       " 'much': 14,\n",
       " 'gonna': 15,\n",
       " 'die': 16}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_3.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1,\n",
       " 'i': 10,\n",
       " 'love': 4,\n",
       " 'you': 8,\n",
       " 'hate': 2,\n",
       " 'are': 2,\n",
       " 'a': 4,\n",
       " 'piece': 2,\n",
       " 'of': 2,\n",
       " 'shit': 2,\n",
       " 'and': 2,\n",
       " 'god': 4,\n",
       " 'facebook': 2,\n",
       " 'is': 2,\n",
       " 'life': 2,\n",
       " 'am': 2,\n",
       " 'so': 2,\n",
       " 'much': 2,\n",
       " 'gonna': 2,\n",
       " 'die': 2}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_3.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming, incremental fitting (new words (not in current vocabulary) which are accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac2c590>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I I love love you you I I hate hate you you'.split(), 'You You are are a a piece piece of of shit shit and and a a god god'.split(),\n",
    "               'Facebook Facebook is is life life I I am am god god'.split(), 'I I love love you you so so much much I I gonna gonna die die'.split(),\n",
    "              'I I love love god god and and hate hate Facebook Facebook'.split()]\n",
    "trial_vectorizer_4 = nlp.cooc_vec.CoocVectorizer(min_df=2, max_df=4, cooc_likelihood=False, normalize=None) #counts only\n",
    "trial_vectorizer_4.fit(trial_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1,\n",
       " 'i': 12,\n",
       " 'love': 6,\n",
       " 'you': 8,\n",
       " 'hate': 4,\n",
       " 'are': 2,\n",
       " 'a': 4,\n",
       " 'piece': 2,\n",
       " 'of': 2,\n",
       " 'shit': 2,\n",
       " 'and': 4,\n",
       " 'god': 6,\n",
       " 'facebook': 4,\n",
       " 'is': 2,\n",
       " 'life': 2,\n",
       " 'am': 2,\n",
       " 'so': 2,\n",
       " 'much': 2,\n",
       " 'gonna': 2,\n",
       " 'die': 2}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_4.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hello', 'i', 'love', 'you', 'god'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_4.rejected_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_4.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Updating Cooccurrence Matrix...\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac2c590>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_4.fit(['She she has has curves curves'.split()]) # 3 new words accepted because counts in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_4.vocabulary_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming, incremental fitting (words (in current vocabulary) which are rejected previously but accepted in this run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac0dcd0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I I love love you you I I hate hate you you'.split(), 'You You are are a a piece piece of of shit shit and and a a god god'.split(),\n",
    "               'Facebook Facebook is is life life I I am am god god'.split(), 'I I love love you you so so much much I I gonna gonna die die'.split(),\n",
    "              'I I love love god god and and hate hate Facebook Facebook'.split()]\n",
    "trial_vectorizer_5 = nlp.cooc_vec.CoocVectorizer(min_df=2, max_df=4, cooc_likelihood=False, normalize=None) #counts only\n",
    "trial_vectorizer_5.fit(trial_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hello', 'i', 'love', 'you', 'god'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.rejected_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 1,\n",
       " 'i': 12,\n",
       " 'love': 6,\n",
       " 'you': 8,\n",
       " 'hate': 4,\n",
       " 'are': 2,\n",
       " 'a': 4,\n",
       " 'piece': 2,\n",
       " 'of': 2,\n",
       " 'shit': 2,\n",
       " 'and': 4,\n",
       " 'god': 6,\n",
       " 'facebook': 4,\n",
       " 'is': 2,\n",
       " 'life': 2,\n",
       " 'am': 2,\n",
       " 'so': 2,\n",
       " 'much': 2,\n",
       " 'gonna': 2,\n",
       " 'die': 2}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hate': 0,\n",
       " 'are': 1,\n",
       " 'a': 2,\n",
       " 'piece': 3,\n",
       " 'of': 4,\n",
       " 'shit': 5,\n",
       " 'and': 6,\n",
       " 'facebook': 7,\n",
       " 'is': 8,\n",
       " 'life': 9,\n",
       " 'am': 10,\n",
       " 'so': 11,\n",
       " 'much': 12,\n",
       " 'gonna': 13,\n",
       " 'die': 14}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Updating Cooccurrence Matrix...\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009ac0dcd0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.fit(['Hello I love you'.split()]) # only hello is accepted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 2,\n",
       " 'i': 13,\n",
       " 'love': 7,\n",
       " 'you': 9,\n",
       " 'hate': 4,\n",
       " 'are': 2,\n",
       " 'a': 4,\n",
       " 'piece': 2,\n",
       " 'of': 2,\n",
       " 'shit': 2,\n",
       " 'and': 4,\n",
       " 'god': 6,\n",
       " 'facebook': 4,\n",
       " 'is': 2,\n",
       " 'life': 2,\n",
       " 'am': 2,\n",
       " 'so': 2,\n",
       " 'much': 2,\n",
       " 'gonna': 2,\n",
       " 'die': 2}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['i', 'love', 'you', 'god'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.rejected_words.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming, incremental fitting (words (in current vocabulary) which are not rejected previously but rejected in this run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009abee1d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_sents = ['Hello I I love love you you I I hate hate you you'.split(), 'You You are are a a piece piece of of shit shit and and a a god god'.split(),\n",
    "               'Facebook Facebook is is life life I I am am god god'.split(), 'I I love love you you so so much much I I gonna gonna die die'.split(),\n",
    "              'I I love love god god and and hate hate Facebook Facebook'.split()]\n",
    "trial_vectorizer_6 = nlp.cooc_vec.CoocVectorizer(min_df=2, max_df=4, cooc_likelihood=False, normalize=None) #counts only\n",
    "trial_vectorizer_6.fit(trial_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['i', 'love', 'you', 'god'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.rejected_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hello': 2,\n",
       " 'i': 13,\n",
       " 'love': 7,\n",
       " 'you': 9,\n",
       " 'hate': 4,\n",
       " 'are': 2,\n",
       " 'a': 4,\n",
       " 'piece': 2,\n",
       " 'of': 2,\n",
       " 'shit': 2,\n",
       " 'and': 4,\n",
       " 'god': 6,\n",
       " 'facebook': 4,\n",
       " 'is': 2,\n",
       " 'life': 2,\n",
       " 'am': 2,\n",
       " 'so': 2,\n",
       " 'much': 2,\n",
       " 'gonna': 2,\n",
       " 'die': 2}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hate': 0,\n",
       " 'are': 1,\n",
       " 'a': 2,\n",
       " 'piece': 3,\n",
       " 'of': 4,\n",
       " 'shit': 5,\n",
       " 'and': 6,\n",
       " 'facebook': 7,\n",
       " 'is': 8,\n",
       " 'life': 9,\n",
       " 'am': 10,\n",
       " 'so': 11,\n",
       " 'much': 12,\n",
       " 'gonna': 13,\n",
       " 'die': 14,\n",
       " 'hello': 15}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_5.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting of Vectorizer Starts...\n",
      "\n",
      "Building A Dictionary of Occurrences for Rejected Words for Storage...\n",
      "\n",
      "Swapping Words with Their Respective Indices...\n",
      "\n",
      "\n",
      "Done...\n",
      "\n",
      "\n",
      "Updating Cooccurrence Matrix...\n",
      "\n",
      "Building Co-Occurence Matrix...\n",
      "Done...\n",
      "\n",
      "Done...\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<deepART.nlp.cooc_vec.cooccurrence.CoocVectorizer at 0x7f009abee1d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_vectorizer_6.fit(['I hate Facebook'.split()]) # hate and facebook are rejected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_vectorizer_4.save('trial.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_vectorizer2 = base.load_model('trial.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
